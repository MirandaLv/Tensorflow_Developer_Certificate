Week 3 Quiz
LATEST SUBMISSION GRADE
100%
1.
Question 1
Why does sequence make a large difference when determining semantics of language?

1 / 1 point

Because the order in which words appear dictate their impact on the meaning of the sentence (Yes)


Because the order in which words appear dictate their meaning


Because the order of words doesn’t matter


It doesn’t

Correct
2.
Question 2
How do Recurrent Neural Networks help you understand the impact of sequence on meaning?

1 / 1 point

They shuffle the words evenly


They look at the whole sentence at a time


They carry meaning from one cell to the next (Yes)


They don’t

Correct
3.
Question 3
How does an LSTM help understand meaning when words that qualify each other aren’t necessarily beside each other in a sentence?

1 / 1 point

They don’t


They load all words into a cell state


Values from earlier words can be carried to later ones via a cell state (Yes)


They shuffle the words randomly

Correct
4.
Question 4
What keras layer type allows LSTMs to look forward and backward in a sentence?

1 / 1 point

Bothdirection


Bilateral


Unilateral


Bidirectional (Yes)

Correct
5.
Question 5
What’s the output shape of a bidirectional LSTM layer with 64 units?

1 / 1 point

(128,None)


(128,1)


(None, 128) (Yes)


(None, 64)

Correct
6.
Question 6
When stacking LSTMs, how do you instruct an LSTM to feed the next one in the sequence?

1 / 1 point

Ensure that return_sequences is set to True on all units


Do nothing, TensorFlow handles this automatically


Ensure that they have the same number of units


Ensure that return_sequences is set to True only on units that feed to another LSTM (Yes)

Correct
7.
Question 7
If a sentence has 120 tokens in it, and a Conv1D with 128 filters with a Kernal size of 5 is passed over it, what’s the output shape?

1 / 1 point

(None, 120, 128)


(None, 116, 128) (Yes)


(None, 120, 124)


(None, 116, 124)

Correct
8.
Question 8
What’s the best way to avoid overfitting in NLP datasets?

1 / 1 point

Use LSTMs


Use GRUs


Use Conv1D


None of the above (Yes)

Correct
